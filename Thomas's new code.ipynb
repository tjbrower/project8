{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.526,\n",
       " 3.585,\n",
       " 3.521,\n",
       " 3.413,\n",
       " 3.422,\n",
       " 2.697,\n",
       " 2.992,\n",
       " 2.414,\n",
       " 2.267,\n",
       " 2.611,\n",
       " 2.815,\n",
       " 2.418,\n",
       " 2.135,\n",
       " 1.913,\n",
       " 1.592,\n",
       " 1.4,\n",
       " 1.525,\n",
       " 1.555,\n",
       " 1.587,\n",
       " 1.629,\n",
       " 1.475,\n",
       " 1.598,\n",
       " 1.139,\n",
       " 0.997,\n",
       " 1.326,\n",
       " 1.075,\n",
       " 0.938,\n",
       " 1.055,\n",
       " 1.089,\n",
       " 1.32,\n",
       " 1.223,\n",
       " 1.152,\n",
       " 1.104,\n",
       " 1.049,\n",
       " 1.097,\n",
       " 0.972,\n",
       " 1.045,\n",
       " 1.039,\n",
       " 1.914,\n",
       " 1.76,\n",
       " 1.554,\n",
       " 1.5,\n",
       " 1.188,\n",
       " 1.888,\n",
       " 1.844,\n",
       " 1.823,\n",
       " 1.425,\n",
       " 1.375,\n",
       " 1.875,\n",
       " 1.125,\n",
       " 1.719,\n",
       " 0.938,\n",
       " 0.975,\n",
       " 1.042,\n",
       " 0.875,\n",
       " 0.831,\n",
       " 0.875,\n",
       " 0.853,\n",
       " 0.803,\n",
       " 0.6,\n",
       " 0.757,\n",
       " 0.75,\n",
       " 0.861,\n",
       " 0.761,\n",
       " 0.735,\n",
       " 0.784,\n",
       " 0.844,\n",
       " 0.813,\n",
       " 0.85,\n",
       " 1.292,\n",
       " 0.825,\n",
       " 0.952,\n",
       " 0.75,\n",
       " 0.675,\n",
       " 1.375,\n",
       " 1.775,\n",
       " 1.021,\n",
       " 1.083,\n",
       " 1.125,\n",
       " 1.313,\n",
       " 1.625,\n",
       " 1.125,\n",
       " 1.125,\n",
       " 1.375,\n",
       " 1.188,\n",
       " 0.982,\n",
       " 1.188,\n",
       " 1.625,\n",
       " 1.375,\n",
       " 5.00001,\n",
       " 1.625,\n",
       " 1.375,\n",
       " 1.625,\n",
       " 1.875,\n",
       " 1.792,\n",
       " 1.3,\n",
       " 1.838,\n",
       " 1.25,\n",
       " 1.7,\n",
       " 1.931,\n",
       " 2.578,\n",
       " 2.734,\n",
       " 2.375,\n",
       " 3.5,\n",
       " 3.357,\n",
       " 3.134,\n",
       " 2.685,\n",
       " 2.594,\n",
       " 2.757,\n",
       " 2.25,\n",
       " 2.625,\n",
       " 2.185,\n",
       " 2.55,\n",
       " 2.241,\n",
       " 2.431,\n",
       " 2.316,\n",
       " 2.185,\n",
       " 2.341,\n",
       " 3.276,\n",
       " 3.476,\n",
       " 3.661,\n",
       " 3.35,\n",
       " 3.736,\n",
       " 3.895,\n",
       " 3.911,\n",
       " 3.373,\n",
       " 2.952,\n",
       " 2.923,\n",
       " 4.115,\n",
       " 3.115,\n",
       " 3.259,\n",
       " 3.926,\n",
       " 3.193,\n",
       " 3.333,\n",
       " 3.352,\n",
       " 3.512,\n",
       " 3.689,\n",
       " 3.659,\n",
       " 3.667,\n",
       " 3.628,\n",
       " 4.833,\n",
       " 3.314,\n",
       " 3.235,\n",
       " 2.167,\n",
       " 2.331,\n",
       " 2.964,\n",
       " 2.737,\n",
       " 2.277,\n",
       " 1.996,\n",
       " 2.398,\n",
       " 2.701,\n",
       " 3.021,\n",
       " 2.695,\n",
       " 3.147,\n",
       " 3.901,\n",
       " 4.103,\n",
       " 3.524,\n",
       " 2.873,\n",
       " 3.487,\n",
       " 2.438,\n",
       " 2.115,\n",
       " 2.184,\n",
       " 2.699,\n",
       " 2.188,\n",
       " 2.25,\n",
       " 2.5,\n",
       " 1.714,\n",
       " 1.938,\n",
       " 1.25,\n",
       " 0.975,\n",
       " 1.25,\n",
       " 1.388,\n",
       " 1.167,\n",
       " 1.518,\n",
       " 1.27,\n",
       " 1.286,\n",
       " 1.406,\n",
       " 1.827,\n",
       " 1.469,\n",
       " 1.228,\n",
       " 1.693,\n",
       " 1.266,\n",
       " 1.279,\n",
       " 1.235,\n",
       " 1.119,\n",
       " 1.128,\n",
       " 1.079,\n",
       " 1.375,\n",
       " 1.055,\n",
       " 0.955,\n",
       " 1.161,\n",
       " 1.122,\n",
       " 0.75,\n",
       " 1.125,\n",
       " 1.25,\n",
       " 1.15,\n",
       " 0.95,\n",
       " 0.964,\n",
       " 0.72,\n",
       " 0.713,\n",
       " 0.808,\n",
       " 1.288,\n",
       " 1.125,\n",
       " 1.194,\n",
       " 1.181,\n",
       " 1.225,\n",
       " 1.063,\n",
       " 1.325,\n",
       " 1.227,\n",
       " 1.133,\n",
       " 1.095,\n",
       " 1.647,\n",
       " 1.25,\n",
       " 1.247,\n",
       " 1.367,\n",
       " 1.417,\n",
       " 1.5,\n",
       " 1.388,\n",
       " 1.392,\n",
       " 1.438,\n",
       " 1.565,\n",
       " 1.51,\n",
       " 2.73,\n",
       " 2.171,\n",
       " 1.871,\n",
       " 2.396,\n",
       " 2.297,\n",
       " 2.573,\n",
       " 2.469,\n",
       " 1.799,\n",
       " 1.696,\n",
       " 2.033,\n",
       " 2.634,\n",
       " 2.318,\n",
       " 1.405,\n",
       " 1.26,\n",
       " 1.22,\n",
       " 1.347,\n",
       " 1.394,\n",
       " 1.154,\n",
       " 1.373,\n",
       " 1.37,\n",
       " 1.426,\n",
       " 1.375,\n",
       " 1.117,\n",
       " 1.268,\n",
       " 1.113,\n",
       " 1.148,\n",
       " 1.023,\n",
       " 1.319,\n",
       " 1.191,\n",
       " 0.806,\n",
       " 0.806,\n",
       " 0.888,\n",
       " 1.024,\n",
       " 0.987,\n",
       " 1.0,\n",
       " 0.923,\n",
       " 0.948,\n",
       " 1.02,\n",
       " 1.584,\n",
       " 1.216,\n",
       " 1.291,\n",
       " 1.214,\n",
       " 1.028,\n",
       " 1.6,\n",
       " 1.439,\n",
       " 1.512,\n",
       " 1.644,\n",
       " 1.565,\n",
       " 2.25,\n",
       " 1.742,\n",
       " 1.669,\n",
       " 1.635,\n",
       " 2.53,\n",
       " 2.407,\n",
       " 2.081,\n",
       " 1.981,\n",
       " 1.728,\n",
       " 3.367,\n",
       " 3.182,\n",
       " 2.858,\n",
       " 2.93,\n",
       " 3.71,\n",
       " 1.573,\n",
       " 1.353,\n",
       " 1.367,\n",
       " 1.607,\n",
       " 1.406,\n",
       " 1.44,\n",
       " 1.619,\n",
       " 1.563,\n",
       " 1.76,\n",
       " 1.458,\n",
       " 1.419,\n",
       " 0.907,\n",
       " 0.906,\n",
       " 1.079,\n",
       " 1.07,\n",
       " 0.982,\n",
       " 0.895,\n",
       " 0.938,\n",
       " 0.967,\n",
       " 0.975,\n",
       " 0.973,\n",
       " 0.912,\n",
       " 0.914,\n",
       " 0.962,\n",
       " 1.201,\n",
       " 1.0,\n",
       " 0.947,\n",
       " 0.857,\n",
       " 0.893,\n",
       " 1.014,\n",
       " 0.725,\n",
       " 0.741,\n",
       " 0.7,\n",
       " 0.846,\n",
       " 0.818,\n",
       " 0.814,\n",
       " 0.818,\n",
       " 0.858,\n",
       " 0.871,\n",
       " 0.835,\n",
       " 0.797,\n",
       " 0.834,\n",
       " 0.847,\n",
       " 0.8,\n",
       " 0.764,\n",
       " 0.797,\n",
       " 0.824,\n",
       " 0.977,\n",
       " 0.972,\n",
       " 1.016,\n",
       " 0.865,\n",
       " 0.905,\n",
       " 1.076,\n",
       " 0.842,\n",
       " 0.889,\n",
       " 0.807,\n",
       " 0.871,\n",
       " 0.851,\n",
       " 0.82,\n",
       " 0.828,\n",
       " 0.846,\n",
       " 0.821,\n",
       " 0.893,\n",
       " 0.983,\n",
       " 0.923,\n",
       " 0.888,\n",
       " 0.9,\n",
       " 0.955,\n",
       " 0.917,\n",
       " 1.004,\n",
       " 0.948,\n",
       " 1.643,\n",
       " 1.555,\n",
       " 1.969,\n",
       " 2.791,\n",
       " 2.528,\n",
       " 2.754,\n",
       " 2.5,\n",
       " 3.298,\n",
       " 1.955,\n",
       " 2.559,\n",
       " 3.111,\n",
       " 3.3,\n",
       " 2.489,\n",
       " 1.429,\n",
       " 1.263,\n",
       " 1.358,\n",
       " 1.306,\n",
       " 1.079,\n",
       " 1.111,\n",
       " 1.22,\n",
       " 1.18,\n",
       " 0.952,\n",
       " 0.943,\n",
       " 0.925,\n",
       " 0.918,\n",
       " 1.094,\n",
       " 1.295,\n",
       " 1.434,\n",
       " 0.982,\n",
       " 1.031,\n",
       " 2.768,\n",
       " 2.709,\n",
       " 2.462,\n",
       " 2.242,\n",
       " 2.291,\n",
       " 2.116,\n",
       " 2.561,\n",
       " 2.063,\n",
       " 2.213,\n",
       " 2.074,\n",
       " 3.5,\n",
       " 2.182,\n",
       " 2.167,\n",
       " 2.706,\n",
       " 2.614,\n",
       " 2.613,\n",
       " 2.322,\n",
       " 3.754,\n",
       " 3.573,\n",
       " 3.744,\n",
       " 3.588,\n",
       " 4.175,\n",
       " 4.305,\n",
       " 3.629,\n",
       " 3.97,\n",
       " 3.468,\n",
       " 3.271,\n",
       " 2.901,\n",
       " 2.92,\n",
       " 2.709,\n",
       " 3.711,\n",
       " 3.356,\n",
       " 3.713,\n",
       " 3.44,\n",
       " 3.422,\n",
       " 3.709,\n",
       " 3.5,\n",
       " 3.686,\n",
       " 3.918,\n",
       " 4.153,\n",
       " 2.849,\n",
       " 2.875,\n",
       " 2.729,\n",
       " 2.596,\n",
       " 2.545,\n",
       " 2.356,\n",
       " 2.184,\n",
       " 2.292,\n",
       " 1.921,\n",
       " 1.793,\n",
       " 1.875,\n",
       " 1.388,\n",
       " 1.5,\n",
       " 1.406,\n",
       " 1.2,\n",
       " 1.448,\n",
       " 1.448,\n",
       " 2.455,\n",
       " 2.0,\n",
       " 1.737,\n",
       " 1.856,\n",
       " 1.826,\n",
       " 2.279,\n",
       " 2.207,\n",
       " 2.12,\n",
       " 2.714,\n",
       " 2.688,\n",
       " 2.778,\n",
       " 3.325,\n",
       " 3.389,\n",
       " 2.875,\n",
       " 3.846,\n",
       " 3.714,\n",
       " 3.5,\n",
       " 5.00001,\n",
       " 1.75,\n",
       " 2.417,\n",
       " 2.167,\n",
       " 2.113,\n",
       " 2.102,\n",
       " 1.964,\n",
       " 1.917,\n",
       " 1.823,\n",
       " 1.75,\n",
       " 1.659,\n",
       " 1.569,\n",
       " 1.643,\n",
       " 1.5,\n",
       " 1.505,\n",
       " 1.699,\n",
       " 1.474,\n",
       " 1.527,\n",
       " 1.827,\n",
       " 1.793,\n",
       " 1.577,\n",
       " 1.531,\n",
       " 2.5,\n",
       " 2.063,\n",
       " 2.188,\n",
       " 1.982,\n",
       " 2.536,\n",
       " 2.574,\n",
       " 2.895,\n",
       " 2.417,\n",
       " 4.896,\n",
       " 4.462,\n",
       " 4.563,\n",
       " 3.368,\n",
       " 5.00001,\n",
       " 5.00001,\n",
       " 3.769,\n",
       " 3.096,\n",
       " 1.823,\n",
       " 1.727,\n",
       " 1.508,\n",
       " 1.536,\n",
       " 1.063,\n",
       " 1.423,\n",
       " 1.194,\n",
       " 1.479,\n",
       " 1.329,\n",
       " 1.325,\n",
       " 1.269,\n",
       " 1.438,\n",
       " 5.00001,\n",
       " 5.00001,\n",
       " 5.00001,\n",
       " 5.00001,\n",
       " 4.661,\n",
       " 5.00001,\n",
       " 4.578,\n",
       " 4.716,\n",
       " 5.00001,\n",
       " 3.986,\n",
       " 3.407,\n",
       " 2.899,\n",
       " 3.353,\n",
       " 3.33,\n",
       " 2.41,\n",
       " 1.896,\n",
       " 2.063,\n",
       " 2.157,\n",
       " 2.102,\n",
       " 2.088,\n",
       " 1.979,\n",
       " 2.013,\n",
       " 1.908,\n",
       " 2.079,\n",
       " 2.153,\n",
       " 2.129,\n",
       " 2.927,\n",
       " 2.75,\n",
       " 2.0,\n",
       " 1.734,\n",
       " 1.022,\n",
       " 2.208,\n",
       " 2.428,\n",
       " 2.654,\n",
       " 2.19,\n",
       " 2.821,\n",
       " 3.72,\n",
       " 2.258,\n",
       " 2.208,\n",
       " 2.345,\n",
       " 2.542,\n",
       " 3.518,\n",
       " 2.75,\n",
       " 4.054,\n",
       " 2.281,\n",
       " 2.34,\n",
       " 1.625,\n",
       " 2.434,\n",
       " 2.738,\n",
       " 2.485,\n",
       " 2.44,\n",
       " 2.409,\n",
       " 3.063,\n",
       " 2.4,\n",
       " 2.471,\n",
       " 2.866,\n",
       " 2.667,\n",
       " 2.298,\n",
       " 2.293,\n",
       " 2.284,\n",
       " 2.062,\n",
       " 3.507,\n",
       " 2.31,\n",
       " 2.529,\n",
       " 3.539,\n",
       " 2.917,\n",
       " 1.0,\n",
       " 3.416,\n",
       " 4.125,\n",
       " 2.486,\n",
       " 2.549,\n",
       " 2.415,\n",
       " 2.952,\n",
       " 3.404,\n",
       " 2.747,\n",
       " 2.386,\n",
       " 2.664,\n",
       " 3.302,\n",
       " 2.068,\n",
       " 2.092,\n",
       " 1.958,\n",
       " 2.535,\n",
       " 2.418,\n",
       " 2.525,\n",
       " 2.639,\n",
       " 2.27,\n",
       " 2.209,\n",
       " 2.593,\n",
       " 2.325,\n",
       " 2.316,\n",
       " 2.094,\n",
       " 2.232,\n",
       " 2.099,\n",
       " 2.082,\n",
       " 2.038,\n",
       " 1.957,\n",
       " 1.905,\n",
       " 1.81,\n",
       " 1.849,\n",
       " 1.908,\n",
       " 2.422,\n",
       " 1.883,\n",
       " 1.835,\n",
       " 2.042,\n",
       " 2.519,\n",
       " 1.875,\n",
       " 1.982,\n",
       " 2.19,\n",
       " 2.75,\n",
       " 2.454,\n",
       " 1.775,\n",
       " 1.716,\n",
       " 1.833,\n",
       " 1.75,\n",
       " 1.567,\n",
       " 1.575,\n",
       " 1.798,\n",
       " 2.055,\n",
       " 1.789,\n",
       " 2.625,\n",
       " 1.65,\n",
       " 1.597,\n",
       " 1.583,\n",
       " 1.448,\n",
       " 1.663,\n",
       " 1.91,\n",
       " 1.75,\n",
       " 1.375,\n",
       " 1.417,\n",
       " 2.109,\n",
       " 1.983,\n",
       " 1.574,\n",
       " 2.647,\n",
       " 2.184,\n",
       " 2.351,\n",
       " 3.418,\n",
       " 3.673,\n",
       " 2.435,\n",
       " 1.679,\n",
       " 3.712,\n",
       " 2.189,\n",
       " 2.027,\n",
       " 1.862,\n",
       " 1.889,\n",
       " 1.8,\n",
       " 2.027,\n",
       " 1.817,\n",
       " 1.668,\n",
       " 1.837,\n",
       " 2.18,\n",
       " 1.915,\n",
       " 1.947,\n",
       " 1.872,\n",
       " 2.231,\n",
       " 2.713,\n",
       " 1.941,\n",
       " 1.904,\n",
       " 1.891,\n",
       " 1.846,\n",
       " 1.973,\n",
       " 1.926,\n",
       " 1.922,\n",
       " 1.946,\n",
       " 1.935,\n",
       " 2.026,\n",
       " 0.833,\n",
       " 2.126,\n",
       " 1.878,\n",
       " 2.057,\n",
       " 1.929,\n",
       " 2.052,\n",
       " 2.134,\n",
       " 1.836,\n",
       " 1.784,\n",
       " 2.192,\n",
       " 1.831,\n",
       " 1.571,\n",
       " 1.578,\n",
       " 1.617,\n",
       " 1.521,\n",
       " 1.449,\n",
       " 1.799,\n",
       " 1.791,\n",
       " 1.599,\n",
       " 1.614,\n",
       " 1.704,\n",
       " 1.375,\n",
       " 1.784,\n",
       " 1.552,\n",
       " 1.609,\n",
       " 1.673,\n",
       " 3.4,\n",
       " 4.31,\n",
       " 3.011,\n",
       " 2.419,\n",
       " 1.625,\n",
       " 2.992,\n",
       " 3.155,\n",
       " 2.168,\n",
       " 2.181,\n",
       " 1.703,\n",
       " 2.75,\n",
       " 1.841,\n",
       " 1.6,\n",
       " 1.75,\n",
       " 1.975,\n",
       " 1.5,\n",
       " 2.056,\n",
       " 1.703,\n",
       " 1.909,\n",
       " 1.969,\n",
       " 1.74,\n",
       " 1.702,\n",
       " 1.809,\n",
       " 1.505,\n",
       " 1.671,\n",
       " 1.489,\n",
       " 1.964,\n",
       " 1.911,\n",
       " 1.875,\n",
       " 1.806,\n",
       " 1.828,\n",
       " 1.793,\n",
       " 1.794,\n",
       " 2.103,\n",
       " 1.923,\n",
       " 1.981,\n",
       " 1.962,\n",
       " 1.816,\n",
       " 1.869,\n",
       " 1.883,\n",
       " 1.84,\n",
       " 1.847,\n",
       " 1.799,\n",
       " 1.672,\n",
       " 1.654,\n",
       " 1.54,\n",
       " 1.535,\n",
       " 1.354,\n",
       " 1.583,\n",
       " 2.407,\n",
       " 1.807,\n",
       " 2.306,\n",
       " 2.108,\n",
       " 1.653,\n",
       " 2.542,\n",
       " 3.326,\n",
       " 3.5,\n",
       " 1.612,\n",
       " 1.793,\n",
       " 1.543,\n",
       " 1.618,\n",
       " 1.619,\n",
       " 1.654,\n",
       " 1.666,\n",
       " 1.497,\n",
       " 1.621,\n",
       " 1.526,\n",
       " 1.952,\n",
       " 1.906,\n",
       " 1.938,\n",
       " 2.006,\n",
       " 1.5,\n",
       " 1.853,\n",
       " 1.727,\n",
       " 1.741,\n",
       " 1.944,\n",
       " 2.067,\n",
       " 2.045,\n",
       " 1.75,\n",
       " 1.375,\n",
       " 1.872,\n",
       " 1.736,\n",
       " 1.63,\n",
       " 3.316,\n",
       " 1.976,\n",
       " 1.979,\n",
       " 2.281,\n",
       " 1.895,\n",
       " 1.842,\n",
       " 1.809,\n",
       " 1.72,\n",
       " 1.779,\n",
       " 1.731,\n",
       " 1.726,\n",
       " 1.821,\n",
       " 1.795,\n",
       " 1.76,\n",
       " 1.926,\n",
       " 1.827,\n",
       " 1.349,\n",
       " 1.576,\n",
       " 1.909,\n",
       " 1.582,\n",
       " 3.5,\n",
       " 1.582,\n",
       " 1.461,\n",
       " 2.267,\n",
       " 1.987,\n",
       " 2.212,\n",
       " 2.085,\n",
       " 2.042,\n",
       " 1.932,\n",
       " 2.095,\n",
       " 1.649,\n",
       " 1.614,\n",
       " 1.87,\n",
       " 1.904,\n",
       " 1.566,\n",
       " 1.573,\n",
       " 1.681,\n",
       " 1.776,\n",
       " 2.089,\n",
       " 1.692,\n",
       " 1.639,\n",
       " 1.647,\n",
       " 1.569,\n",
       " 1.688,\n",
       " 1.741,\n",
       " 2.065,\n",
       " 2.039,\n",
       " 2.17,\n",
       " 1.596,\n",
       " 1.598,\n",
       " 1.543,\n",
       " 1.496,\n",
       " 1.886,\n",
       " 2.385,\n",
       " 2.353,\n",
       " 2.614,\n",
       " 2.344,\n",
       " 2.317,\n",
       " 1.563,\n",
       " 2.22,\n",
       " 2.544,\n",
       " 2.443,\n",
       " 2.47,\n",
       " 2.365,\n",
       " 1.514,\n",
       " 2.137,\n",
       " 2.436,\n",
       " 2.125,\n",
       " 2.147,\n",
       " 2.132,\n",
       " 2.56,\n",
       " 2.375,\n",
       " 1.781,\n",
       " 2.318,\n",
       " 1.479,\n",
       " 1.917,\n",
       " 2.476,\n",
       " 2.835,\n",
       " 2.169,\n",
       " 2.825,\n",
       " 2.956,\n",
       " 2.818,\n",
       " 2.698,\n",
       " 2.312,\n",
       " 3.183,\n",
       " 2.534,\n",
       " 2.782,\n",
       " 3.0,\n",
       " 2.735,\n",
       " 2.234,\n",
       " 2.145,\n",
       " 2.202,\n",
       " 2.403,\n",
       " 1.58,\n",
       " 2.242,\n",
       " 1.772,\n",
       " 2.161,\n",
       " 2.697,\n",
       " 2.312,\n",
       " 1.625,\n",
       " 2.54,\n",
       " 1.962,\n",
       " 1.894,\n",
       " 2.208,\n",
       " 2.138,\n",
       " 4.429,\n",
       " 3.334,\n",
       " 3.524,\n",
       " 3.053,\n",
       " 2.596,\n",
       " 2.489,\n",
       " 3.56,\n",
       " 2.36,\n",
       " 2.188,\n",
       " 2.28,\n",
       " 2.359,\n",
       " 2.032,\n",
       " 1.97,\n",
       " 2.364,\n",
       " 2.232,\n",
       " 2.227,\n",
       " 2.25,\n",
       " 2.401,\n",
       " 2.279,\n",
       " 2.686,\n",
       " 2.751,\n",
       " 2.563,\n",
       " 3.324,\n",
       " 2.876,\n",
       " 2.989,\n",
       " 2.876,\n",
       " 2.222,\n",
       " 2.326,\n",
       " 2.583,\n",
       " 2.309,\n",
       " 1.969,\n",
       " 2.235,\n",
       " 2.239,\n",
       " 2.435,\n",
       " 4.511,\n",
       " 5.00001,\n",
       " 1.931,\n",
       " 2.644,\n",
       " 2.469,\n",
       " 3.373,\n",
       " 2.848,\n",
       " 2.173,\n",
       " 2.917,\n",
       " 2.238,\n",
       " 2.231,\n",
       " 2.113,\n",
       " 2.185,\n",
       " 1.563,\n",
       " 2.482,\n",
       " 1.86,\n",
       " 1.782,\n",
       " 1.904,\n",
       " 2.103,\n",
       " 2.08,\n",
       " 1.833,\n",
       " 2.457,\n",
       " 2.422,\n",
       " 1.817,\n",
       " 2.994,\n",
       " 2.195,\n",
       " 2.318,\n",
       " 2.213,\n",
       " 2.138,\n",
       " 2.06,\n",
       " 2.711,\n",
       " 3.365,\n",
       " 3.156,\n",
       " 5.00001,\n",
       " 2.854,\n",
       " 3.23,\n",
       " 2.919,\n",
       " 2.256,\n",
       " 2.596,\n",
       " 2.629,\n",
       " 2.573,\n",
       " 2.592,\n",
       " 3.397,\n",
       " 3.372,\n",
       " 3.341,\n",
       " 3.115,\n",
       " 2.969,\n",
       " 2.479,\n",
       " 3.004,\n",
       " 3.324,\n",
       " 3.898,\n",
       " 2.33,\n",
       " 2.385,\n",
       " 2.333,\n",
       " 2.313,\n",
       " 4.758,\n",
       " 3.932,\n",
       " 2.472,\n",
       " 2.294,\n",
       " 2.825,\n",
       " 3.006,\n",
       " 2.932,\n",
       " 2.785,\n",
       " 3.529,\n",
       " 4.0,\n",
       " 3.75,\n",
       " 3.137,\n",
       " 1.393,\n",
       " 3.275,\n",
       " 4.306,\n",
       " 1.25,\n",
       " 4.5,\n",
       " 2.002,\n",
       " 1.924,\n",
       " 4.188,\n",
       " 2.168,\n",
       " 2.155,\n",
       " 2.277,\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spyder\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedInc</th>\n",
       "      <th>Population</th>\n",
       "      <th>classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.023810</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>41.0</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>322.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.971880</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.073446</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>496.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.073059</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>558.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.081081</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>565.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.103627</td>\n",
       "      <td>2.139896</td>\n",
       "      <td>4.761658</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>4.0368</td>\n",
       "      <td>413.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.951362</td>\n",
       "      <td>2.128405</td>\n",
       "      <td>4.931907</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.6591</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.061824</td>\n",
       "      <td>1.788253</td>\n",
       "      <td>4.797527</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.1200</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.117647</td>\n",
       "      <td>2.026891</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>42.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>2.0804</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.990196</td>\n",
       "      <td>2.172269</td>\n",
       "      <td>4.970588</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.6912</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.079602</td>\n",
       "      <td>2.263682</td>\n",
       "      <td>5.477612</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>3.2031</td>\n",
       "      <td>910.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.024523</td>\n",
       "      <td>2.049046</td>\n",
       "      <td>4.772480</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>3.2705</td>\n",
       "      <td>1504.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.012821</td>\n",
       "      <td>2.346154</td>\n",
       "      <td>5.322650</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>3.0750</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.097701</td>\n",
       "      <td>1.982759</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>2.6736</td>\n",
       "      <td>345.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.009677</td>\n",
       "      <td>1.954839</td>\n",
       "      <td>4.262903</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>1.9167</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.071970</td>\n",
       "      <td>2.640152</td>\n",
       "      <td>4.242424</td>\n",
       "      <td>50.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>2.1250</td>\n",
       "      <td>697.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.048338</td>\n",
       "      <td>2.395770</td>\n",
       "      <td>5.939577</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>2.7750</td>\n",
       "      <td>793.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.966997</td>\n",
       "      <td>2.138614</td>\n",
       "      <td>4.052805</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>2.1202</td>\n",
       "      <td>648.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.085919</td>\n",
       "      <td>2.362768</td>\n",
       "      <td>5.343675</td>\n",
       "      <td>50.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>1.9911</td>\n",
       "      <td>990.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.083636</td>\n",
       "      <td>2.509091</td>\n",
       "      <td>5.465455</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>2.6033</td>\n",
       "      <td>690.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.108434</td>\n",
       "      <td>2.463855</td>\n",
       "      <td>4.524096</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>1.3578</td>\n",
       "      <td>409.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.002732</td>\n",
       "      <td>2.538251</td>\n",
       "      <td>4.478142</td>\n",
       "      <td>42.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>1.7135</td>\n",
       "      <td>929.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.131799</td>\n",
       "      <td>2.123431</td>\n",
       "      <td>5.096234</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>1.7250</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.036923</td>\n",
       "      <td>2.624615</td>\n",
       "      <td>5.193846</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>2.1806</td>\n",
       "      <td>853.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.035545</td>\n",
       "      <td>2.383886</td>\n",
       "      <td>5.270142</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>2.6000</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.033613</td>\n",
       "      <td>2.663866</td>\n",
       "      <td>4.495798</td>\n",
       "      <td>41.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.28</td>\n",
       "      <td>2.4038</td>\n",
       "      <td>317.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.020921</td>\n",
       "      <td>2.539749</td>\n",
       "      <td>4.728033</td>\n",
       "      <td>49.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.28</td>\n",
       "      <td>2.4597</td>\n",
       "      <td>607.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.060453</td>\n",
       "      <td>2.775819</td>\n",
       "      <td>4.780856</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.28</td>\n",
       "      <td>1.8080</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.040169</td>\n",
       "      <td>2.391121</td>\n",
       "      <td>4.401691</td>\n",
       "      <td>50.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.28</td>\n",
       "      <td>1.6424</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.032258</td>\n",
       "      <td>2.548387</td>\n",
       "      <td>4.703226</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.28</td>\n",
       "      <td>1.6875</td>\n",
       "      <td>395.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AveBedrms  AveOccup  AveRooms  HouseAge  Latitude  Longitude  MedInc  \\\n",
       "0    1.023810  2.555556  6.984127      41.0     37.88    -122.23  8.3252   \n",
       "1    0.971880  2.109842  6.238137      21.0     37.86    -122.22  8.3014   \n",
       "2    1.073446  2.802260  8.288136      52.0     37.85    -122.24  7.2574   \n",
       "3    1.073059  2.547945  5.817352      52.0     37.85    -122.25  5.6431   \n",
       "4    1.081081  2.181467  6.281853      52.0     37.85    -122.25  3.8462   \n",
       "5    1.103627  2.139896  4.761658      52.0     37.85    -122.25  4.0368   \n",
       "6    0.951362  2.128405  4.931907      52.0     37.84    -122.25  3.6591   \n",
       "7    1.061824  1.788253  4.797527      52.0     37.84    -122.25  3.1200   \n",
       "8    1.117647  2.026891  4.294118      42.0     37.84    -122.26  2.0804   \n",
       "9    0.990196  2.172269  4.970588      52.0     37.84    -122.25  3.6912   \n",
       "10   1.079602  2.263682  5.477612      52.0     37.85    -122.26  3.2031   \n",
       "11   1.024523  2.049046  4.772480      52.0     37.85    -122.26  3.2705   \n",
       "12   1.012821  2.346154  5.322650      52.0     37.85    -122.26  3.0750   \n",
       "13   1.097701  1.982759  4.000000      52.0     37.84    -122.26  2.6736   \n",
       "14   1.009677  1.954839  4.262903      52.0     37.85    -122.26  1.9167   \n",
       "15   1.071970  2.640152  4.242424      50.0     37.85    -122.26  2.1250   \n",
       "16   1.048338  2.395770  5.939577      52.0     37.85    -122.27  2.7750   \n",
       "17   0.966997  2.138614  4.052805      52.0     37.85    -122.27  2.1202   \n",
       "18   1.085919  2.362768  5.343675      50.0     37.84    -122.26  1.9911   \n",
       "19   1.083636  2.509091  5.465455      52.0     37.84    -122.27  2.6033   \n",
       "20   1.108434  2.463855  4.524096      40.0     37.85    -122.27  1.3578   \n",
       "21   1.002732  2.538251  4.478142      42.0     37.85    -122.27  1.7135   \n",
       "22   1.131799  2.123431  5.096234      52.0     37.84    -122.27  1.7250   \n",
       "23   1.036923  2.624615  5.193846      52.0     37.84    -122.27  2.1806   \n",
       "24   1.035545  2.383886  5.270142      52.0     37.84    -122.27  2.6000   \n",
       "25   1.033613  2.663866  4.495798      41.0     37.85    -122.28  2.4038   \n",
       "26   1.020921  2.539749  4.728033      49.0     37.85    -122.28  2.4597   \n",
       "27   1.060453  2.775819  4.780856      52.0     37.85    -122.28  1.8080   \n",
       "28   1.040169  2.391121  4.401691      50.0     37.84    -122.28  1.6424   \n",
       "29   1.032258  2.548387  4.703226      52.0     37.84    -122.28  1.6875   \n",
       "\n",
       "    Population  classifier  \n",
       "0        322.0         NaN  \n",
       "1       2401.0         NaN  \n",
       "2        496.0         NaN  \n",
       "3        558.0         NaN  \n",
       "4        565.0         NaN  \n",
       "5        413.0         NaN  \n",
       "6       1094.0         NaN  \n",
       "7       1157.0         NaN  \n",
       "8       1206.0         NaN  \n",
       "9       1551.0         NaN  \n",
       "10       910.0         NaN  \n",
       "11      1504.0         NaN  \n",
       "12      1098.0         NaN  \n",
       "13       345.0         NaN  \n",
       "14      1212.0         NaN  \n",
       "15       697.0         NaN  \n",
       "16       793.0         NaN  \n",
       "17       648.0         NaN  \n",
       "18       990.0         NaN  \n",
       "19       690.0         NaN  \n",
       "20       409.0         NaN  \n",
       "21       929.0         NaN  \n",
       "22      1015.0         NaN  \n",
       "23       853.0         NaN  \n",
       "24      1006.0         NaN  \n",
       "25       317.0         NaN  \n",
       "26       607.0         NaN  \n",
       "27      1102.0         NaN  \n",
       "28      1131.0         NaN  \n",
       "29       395.0         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "dummy = data.fetch_california_housing()\n",
    "df_x = pd.DataFrame(dummy.data, columns = dummy.feature_names)\n",
    "desiredOutput = dummy.target\n",
    "desiredOutput = list(desiredOutput)\n",
    "df_y = pd.DataFrame(desiredOutput, columns = ['classifier'])\n",
    "df_y.columns = ['classifier']\n",
    "display(list(df_y.classifier.values))\n",
    "full_df = df_x.append()\n",
    "display(full_df.head(30))\n",
    "#display(df_x.head(5), df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
       "       'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.14999"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_x.columns)\n",
    "test_y = df_y.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroToOneValues = df_y.loc[(df_y.classifier > 0.0) & (df_y.classifier <= 1.0)]\n",
    "max(zeroToOneValues.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15480, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5160, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(15480, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5160, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y)\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spyder\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([3.499, 2.22 , 0.869, ..., 1.838, 0.914, 2.73 ]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-fba0f5670a4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Spyder\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    980\u001b[0m         \"\"\"\n\u001b[0;32m    981\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 982\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spyder\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    321\u001b[0m                              hidden_layer_sizes)\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spyder\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spyder\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spyder\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: (array([3.499, 2.22 , 0.869, ..., 1.838, 0.914, 2.73 ]),)"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
